---
layout: post
title: Local LLMs V - Installing and running vLLM
subtitle: In which I go the easy way
date: 2026-01-19
tags:
  - local-llm
  - vllm
image: /assets/images/2026-01-17-vllm/2026-01-17-20-14-36.png
---

Install is easy. Go to https://docs.vllm.ai/en/stable/getting_started/installation/gpu/#install-specific-revisions, create env to not mess up with existing Pytorch and CUDA libraries and
```bash
uv venv --python 3.12 --seed
source .venv/bin/activate
```

As usually, I build the wheel from the source. We can do it with precompiled C++ libraries:
```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
VLLM_USE_PRECOMPILED=1 uv pip install --editable .
```

What we are doing here is create the Python installable package but downloading the precompiled C++ libraries (maybe some C libraries too? Not sure.) It takes 1m33s in my computer.

But I also want to try building all the code locally with
```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
uv pip install -e .
```
This took 21m40s of intense CPU activity.


Latency test:
```bash
vllm bench latency
    --model /home/xavi/.cache/huggingface/hub/models--unsloth--Qwen3-8B-GGUF/snapshots/a6adef130ffb23ddaf1a62fec9dced968c9bc482/Qwen3-8B-Q8_0.gguf
    --dtype float16
    --input-len 512
    --output-len 256
    --max_model_len 24576
```

<figure><img src='/assets/images/2026-01-17-vllm/2026-01-17-20-07-08.png' alt='Image by Nagabhushana Rao Vadlamani' /><figcaption><a href='https://www.researchgate.net/figure/An-illustration-of-the-common-floating-point-formats-used-including-bfloat16-The-number_fig1_363561544'>Image by Nagabhushana Rao Vadlamani</a></figcaption></figure><br/>

Explanation of bfloat16: https://nhigham.com/2020/06/02/what-is-bfloat16-arithmetic/


throughput test
```bash
vllm bench throughput
    --model /home/xavi/.cache/huggingface/hub/models--unsloth--Qwen3-8B-GGUF/snapshots/a6adef130ffb23ddaf1a62fec9dced968c9bc482/Qwen3-8B-Q8_0.gguf
    --dtype float16
    --input-len 512
    --output-len 256
    --max_model_len 16000
```

Throughput: 0.24 requests/s, 180.53 total tokens/s, 60.18 output tokens/s
Total num prompt tokens:  512000
Total num output tokens:  256000


Throughput: 0.24 requests/s, 180.53 total tokens/s, 60.18 output tokens/s
Total num prompt tokens:  512000
Total num output tokens:  256000


