--- layout: page ---

<section>
    <div class="blog-post text-container">
        <p class="post-details">
	
		
		<span class="blog-filter">
			<a href="/category/python/">Python</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/libros/">Libros</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/machine-learning/">Machine Learning</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/inteligencia-artificial/">Inteligencia Artificial</a>
		</span>
	
	
	
	
	
	
	
	<span class="post-date">28 de mayo de 2017</span>
</p>


        <div class="post-content">
            <p>A partir del concepto de <a href="/2017/05/21/python-machine-learning-vii/">regresión logística</a> 
podemos ya definir el siguiente algoritmo.</p>

<h2 id="el-modelo-para-la-regresión-logística">El modelo para la regresión logística</h2>

<p>El libro traza un paralelo con el modelo <a href="/2017/04/02/python-machine-learning-iii/">ADALINE</a>, donde la función indentidad (\(\phi(z)=z\)) es sustituida por la función logística (sigmoidea) en nuestro modelo:</p>

<div style="text-align:center">
    <figure>
        <img alt="El algoritmo de ADALINE con la función identidad sustituida por la función logística o sigmoidea" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch03/images/03_03.png" />
        <figcaption>El algoritmo de ADALINE con la función identidad sustituida por la función logística o sigmoidea</figcaption>
    </figure>
</div>

<h3 id="cálculo-de-los-pesos">Cálculo de los pesos</h3>

<p>Cuando hablamos de ADALINE definimos la <a href="/2017/04/02/python-machine-learning-iii/#1">función de costos de la suma de los errores cuadrados</a> para minimizarla:</p>

\[J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2.\]

<p>Para obtener una función de costos que minimizar, el autor empieza definiendo la siguiente función:</p>

\[L(\mathbf{w}) = P(\mathbf{y} | \mathbf{x}; \mathbf{w}) = \prod_{i=1}^{n} P \big( y^{(i)} | x^{(i)}; \mathbf{w} \big) =  \prod_{i=1}^{n} \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}},\]

<p>donde \(y^{(i)}\) es la clase (\(1\) o \(0\)) de la muestra \(i\), y \(\phi \big(z^{(i)} \big)\) es la probabilidad de que la muestra \(i\) pertenezca a la clase \(1\). Como vimos en la entrada anterior, la clase predicha \(\hat y\) será \(1\) si la probabilidad de es mayor o igual a \(0.5\).</p>

<p>Por tanto, la expresión</p>

\[\bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}\]

<p>indica la probabilidad de que la muestra \(i\) pertenezca a la clase real (\(y^{(i)}\)), ya que si \(y^{(i)} = 1\), la expresión se simplifica a \(\phi \big(z^{(i)} \big)\), y si \(y^{(i)} = 0\) nos queda</p>

\[\bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}},\]

<p>que es la probabilidad de que la muestra sea de clase distinta de \(1\), es decir, la probabilidad de que se de clase \(0\).</p>

<p>Entonces, la función \(L(\mathbf{w})\) es el producto de las probabilidades de que cada muestra sea de su clase real, en función de los pesos \(\mathbf{w}\) establecidos. Si maximizamos esa función, estaremos maximizando la calidad de la predicción para el conjunto de muestras.</p>

<h3 id="simplificando-los-cálculos">Simplificando los cálculos</h3>

<p>El siguiente paso que da el libro es calcular el logaritmo de \(L(\mathbf{w})\). El propósito de esta función es doble. Por un lado, evita un problema potencial si las probabilidades son lo suficientemente pequeñas para que su producto provoque un desbordamiento. El logaritmo convierte los productos en sumas, con lo que el riesgo desaparece. Por otro lado, el cálculo de la derivada se simplifica mucho por el mismo motivo.</p>

\[l(\mathbf{w}) = \log L(\mathbf{w}) = \log \Bigg[ \prod_{i=1}^{n} \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}  \Bigg]\]

<p>Dado que \(\log (xy) = \log x + \log y\), el logaritmo del productorio se convierte en un sumatorio de logaritmos:</p>

\[l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ \log \bigg( \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}} \bigg) \Bigg].\]

<p>Ahora el sumatorio contiene también el logaritmo de un producto, por lo que podemos convertirlo en:</p>

\[l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ \log \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} + \log \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}  \Bigg].\]

<p>Finalmente podemos simplificar usar la propiedad \(\log (x^y) = y \log x\) para convertir la expresión en:</p>

\[l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ y^{(i)} \log \bigg( \phi \big(z^{(i)} \big) \bigg) + \bigg(1-y^{(i)} \bigg) \log \bigg( 1 - \phi \big( z^{(i)} \big) \bigg) \Bigg].\]

<p>En la siguiente entrada veremos cómo convertir esta expresión en una función de costos.</p>


            <div class="blog-navigation">
                
                <a class="prev" href="/2017/05/21/python-machine-learning-vii/">&laquo; PML T3 - Regresión logística</a>  
                <a class="next" href="/2018/02/26/deep-learning-00/">Deep learning - Prólogo &raquo;</a> 
            </div>
        </div>
    </div>
</section>